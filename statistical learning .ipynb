{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regresion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Is there a relationship\n",
    "# 2 .How strong is the relationship\n",
    "# 3 Which factor effects the most ?\n",
    "# 4.How accurately can we estimate the effect of each factor on the outcome?\n",
    "# 5.How accurately can we predict future of teh outcome?\n",
    "# 6.Is the relationship linear?\n",
    "# 7. Is there synergy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Simple Linear Regression\n",
    "#Y ≈ β0 + β1X.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Equation 3.1, β0 and β1 are two unknown constants that represent the intercept and slope terms in the linear model. Together, β0 and β1 are known as the model coefficients or parameters\n",
    "# yˆ = βˆ 0 + βˆ 1 x ,\n",
    "# 3.1.1 Estimating the Coefficients\n",
    "# (x1,y1), (x2,y2),..., (xn,yn)\n",
    "# represent n observation pairs, each of which consists of a measurement of X and a measurement of Y .\n",
    "# Let yˆi = βˆ0 + βˆ1xi be the prediction for Y based on the ith value of X. Then ei = yi − yˆi represents the ith residual—this is the difference between the ith observed response value and the ith response value that is predicted by our linear model. We define the residual sum of squares (RSS) as\n",
    "# residual\n",
    "# residual sum of squares\n",
    "# R S S = e 21 + e 2 2 + · · · + e 2n ,\n",
    "\n",
    "# RSS = (y1−βˆ0−βˆ1x1)2+(y2−βˆ0−βˆ1x2)2+...+(yn−βˆ0−βˆ1xn)2. (3.3)\n",
    "\n",
    "\n",
    "# The least squares approach chooses βˆ0 and βˆ1 to minimize the RSS. Using some calculus, one can show that the minimizers are\n",
    "# 􏰂ni=1(xi − x ̄)(yi − y ̄)\n",
    "# β 1 = 􏰂 ni = 1 ( x i − x ̄ ) 2 , ( 3 . 4 )\n",
    "# βˆ 0 = y ̄ − βˆ 1 x ̄ , refer to book at pg 62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #3.1.2 Assessing the Accuracy of the Coefficient Estimates\n",
    "# Recall from (2.1) that we assume that the true relationship between X and Y takes the form Y = f(X) + ε for some unknown function f, where ε is a mean-zero random error term. If f is to be approximated by a linear function, then we can write this relationship as\n",
    "# Y = β0 + β1X + ε.A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how accurate is the sample mean μˆ as an estimate of μ?\n",
    "\n",
    "# we answer this question by computing the standard error of μˆ, written as SE(μˆ). We have the well-known formula\n",
    "# 2 σ2\n",
    "# Var(μˆ) = SE(μˆ) = n , (3.7)\n",
    "# bias unbiased\n",
    "\n",
    "# where σ is the standard deviation of each of the realizations yi of Y .2 Roughly speaking, the standard error tells us the average amount that this estimate μˆ differs from the actual value of μ.\n",
    "\n",
    "# where σ2 = Var(ε).\n",
    "# The estimate\n",
    "# of σ is known as the residual standard error, and is given by the formula 􏰎2\n",
    "# residual standard error\n",
    "# confidence interval\n",
    "#  RSE = RSS/(n − 2)\n",
    "    \n",
    "#     For linear regression, the 95% confidence interval for β1 approximately takes the form\n",
    "# βˆ1 ± 2 · SE(βˆ1).\n",
    "\n",
    "# That is, there is approximately a 95 % chance that the interval\n",
    "# 􏰐􏰑\n",
    "# βˆ1 − 2 · SE(βˆ1), βˆ1 + 2 · SE(βˆ1) (3.10) will contain the true value of β1.3 Similarly, a confidence interval for β0\n",
    "# approximately takes the form\n",
    "# βˆ0 ± 2 · SE(βˆ0).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard errors can also be used to perform hypothesis tests on the coefficients. The most common hypothesis test involves testing the null hypothesis of\n",
    "# hypothesis test\n",
    "# null hypothesis\n",
    "# alternative hypothesis\n",
    "# H0 : There is no relationship between X and Y versus the alternative hypothesis\n",
    "# Ha : There is some relationship between X and Y . Mathematically, this corresponds to testing\n",
    "# (3.12)\n",
    "# (3.13)\n",
    "# versus\n",
    "# H0 :β1 =0 Ha :β1 ̸=0,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
